{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2 Ingeniería Linguística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_glossary():\n",
    "    file_deportes = open(\"./Glosarios/Glosario_deportes/glosario_deportes.txt\",encoding='utf-8')\n",
    "    glosario_deportes = file_deportes.read().split(\"\\n\")\n",
    "    \n",
    "    file_politica = open(\"./Glosarios/Glosario_politica/glosario_politica.txt\",encoding='utf-8')\n",
    "    glosario_politica = file_politica.read().split(\"\\n\")\n",
    "    \n",
    "    file_salud = open(\"./Glosarios/Glosario_salud/glosario_salud.txt\",encoding='utf-8')\n",
    "    glosario_salud = file_salud.read().split(\"\\n\")\n",
    "    \n",
    "    glosarios = [glosario_deportes,glosario_politica,glosario_salud]\n",
    "    \n",
    "    return glosarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glosario = open_glossary()\n",
    "glosario_deportes = glosario[0]\n",
    "glosario_politica = glosario[1]\n",
    "glosario_salud = glosario[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(filenames):\n",
    "    documents = []\n",
    "    for i in filenames:\n",
    "        file_document = open(i,encoding='utf-8')\n",
    "        documents.append(file_document.read().split())\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_deportes = glob.glob('./Documentos/Deportes/*')\n",
    "filenames_politica = glob.glob('./Documentos/Politica/*')\n",
    "filenames_salud = glob.glob('./Documentos/Salud/*')\n",
    "\n",
    "\n",
    "documentos_deportes = load_documents(filenames_deportes)\n",
    "documentos_politica = load_documents(filenames_politica)\n",
    "documentos_salud = load_documents(filenames_salud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "#nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "articles = load_files(r\"./Documentos/\",encoding='utf-8')\n",
    "X, y = articles.data, articles.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 2 0 1 1 1 2 0 0 1 2 0 0 0 2 2 1 0 0 2 1 2 1 2 2 2 0 2 2 0 0 1 1 2\n",
      " 1 1 0 0 0 1 1 0 2 2 0 1 0 1 0 1 0 1 0 1 2 1 2 0 1 2 0 2 0 0 1 2 2 2 0 2 2\n",
      " 0 1 1 1 2 1 0 2 1 0 2 0 2 2 2 1 1]\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(len(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for d in range(0, len(X)):\n",
    "    document = re.sub(r'\\W', ' ', str(X[d]))\n",
    "    document = document.lower()\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(ngram_range=(1, 2),\n",
      "                vocabulary=[['ganar', 'derbi', 'Real Madrid', 'Madrid',\n",
      "                             'Atlético de Madrid', 'Atleti', 'olímpicos',\n",
      "                             'final', 'rival', 'partido', 'partidos',\n",
      "                             'deportivo', 'penalti', 'liga', 'fútbol', 'equipo',\n",
      "                             'equipos', 'aficionados', 'futbolistas', 'torneo',\n",
      "                             'club', 'balón', 'gol'],\n",
      "                            ['artículo', 'gobierno', 'ley', 'constitución',\n",
      "                             'reforma', 'mayoría', 'debate', 'parte', 'rey',\n",
      "                             'congreso', 'comisión', 'país', 'derecho',\n",
      "                             'ministerio', 'públicos', 'comunidad', 'partido',\n",
      "                             'laboral', 'política', 'electoral'],\n",
      "                            ['asma', 'defectos', 'medicamentos', 'salud',\n",
      "                             'azúcar', 'caso', 'estudio', 'riesgo',\n",
      "                             'medicación', 'embarazo', 'cirujía', 'síntomas',\n",
      "                             'dolor', 'mujeres', 'enfermedad', 'problema',\n",
      "                             'factores', 'pandemia', 'niños', 'productos']])\n"
     ]
    }
   ],
   "source": [
    "vectorizer_deportes = CountVectorizer(vocabulary = glosario[0],ngram_range=(1, 2))\n",
    "vectorizer_politica = CountVectorizer(vocabulary = glosario[1],ngram_range=(1, 2))\n",
    "vectorizer_salud = CountVectorizer(vocabulary = glosario[2],ngram_range=(1, 2))\n",
    "\n",
    "\n",
    "vocabulario = {0: glosario[0],1: glosario[1],2:glosario[2]}\n",
    "\n",
    "\n",
    "vectorizer_ = CountVectorizer(vocabulary = glosario,ngram_range=(1, 2))\n",
    "\n",
    "print(vectorizer_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 0 0 1 0 2 0 0 2 0 0 0 0 0 5 5]\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X_deportes = vectorizer_deportes.fit_transform(documents).toarray()\n",
    "X_politica = vectorizer_politica.fit_transform(documents).toarray()\n",
    "X_salud = vectorizer_salud.fit_transform(documents).toarray()\n",
    "\n",
    "print((X_deportes[0]))\n",
    "print((X_politica[0]))\n",
    "print((X_salud[0]))\n",
    "\n",
    "X = [X_deportes,X_politica,X_salud]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_glosario = [1,1,1,1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "dataSetI = [3, 45, 7, 2]\n",
    "dataSetII = [2, 54, 13, 15]\n",
    "result = 1 - spatial.distance.cosine(dataSetI, dataSetII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0802526  0.         0.         0.09030799 0.\n",
      " 0.31301098 0.         0.         0.18398048 0.         0.\n",
      " 0.         0.         0.         0.6766357  0.62908142]\n"
     ]
    }
   ],
   "source": [
    "tfidfconverter = TfidfTransformer()\n",
    "X_deportes = tfidfconverter.fit_transform(X_).toarray()\n",
    "print(X_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulario = {0: glosario[0],1: glosario[1],2:glosario[2]}\n",
    "type(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-207-ccd1eb261a6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mTfidf_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulario\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mTfidf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mTrain_X_Tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidf_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1815\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1816\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1817\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1818\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1819\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m         \u001b[0mmax_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_validate_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m                 \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vocabulary contains repeated indices.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(documents, y, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "vocabulario = {0: glosario[0],1: glosario[1],2:glosario[2]}\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(vocabulary = vocabulario)\n",
    "Tfidf_vect.fit(documents)\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test)\n",
    "\n",
    "Modelo = SGDClassifier(loss='hinge',penalty='l2',alpha=1e-3, max_iter=20, random_state=42)\n",
    "Modelo.fit(Train_X_Tfidf, y_train)\n",
    "y_pred = Modelo.predict(Test_X_Tfidf)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6086956521739131\n",
      "[1 1 0 0 2 0 2 1 0 1 1 2 0 1 1 1 0 0 0 1 1 1 2 1 0 2 0 0 2 2 0 1 0 0 0 2 1\n",
      " 0 2 0 0 2 2 0 2 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fernando\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(documents, y, test_size=0.5, random_state=0)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(vocabulary = glosario[1])\n",
    "Tfidf_vect.fit(documents)\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test)\n",
    "\n",
    "Modelo = SGDClassifier(loss='hinge',penalty='l2',alpha=1e-3, max_iter=20, random_state=42)\n",
    "Modelo.fit(Train_X_Tfidf, y_train)\n",
    "y_pred = Modelo.predict(Test_X_Tfidf)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5869565217391305\n",
      "[1 1 0 0 0 2 0 1 2 0 0 2 0 1 1 0 0 1 0 1 1 2 2 1 2 1 0 0 2 0 2 1 2 2 1 2 0\n",
      " 1 1 1 0 2 0 1 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fernando\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(documents, y, test_size=0.5, random_state=0)\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(vocabulary = glosario[2])\n",
    "Tfidf_vect.fit(documents)\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test)\n",
    "\n",
    "Modelo = SGDClassifier(loss='hinge',penalty='l2',alpha=1e-3, max_iter=20, random_state=42)\n",
    "Modelo.fit(Train_X_Tfidf, y_train)\n",
    "y_pred = Modelo.predict(Test_X_Tfidf)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, test_size=0.5, random_state=0)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6521739130434783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fernando\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    }
   ],
   "source": [
    "classifier =SGDClassifier(loss='hinge',penalty='l2',alpha=1e-3, max_iter=20, random_state=42)\n",
    "classifier.fit(X_train, y_train) \n",
    "y_pred = classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3695652173913043\n"
     ]
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train) \n",
    "y_pred = classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15217391304347827\n"
     ]
    }
   ],
   "source": [
    "classifier = KMeans()\n",
    "classifier.fit(X_train, y_train) \n",
    "y_pred = classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6521739130434783\n"
     ]
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(3)\n",
    "classifier.fit(X_train, y_train) \n",
    "y_pred = classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
